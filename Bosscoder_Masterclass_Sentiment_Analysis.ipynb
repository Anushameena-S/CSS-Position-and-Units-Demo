{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anushameena-S/CSS-Position-and-Units-Demo/blob/main/Bosscoder_Masterclass_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ormb1VoGVu_U",
        "outputId": "11c08e30-e588-446e-8ced-6f4a8881995a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome everyone\n",
            "Lets start at 8:05 pm\n"
          ]
        }
      ],
      "source": [
        "print (\"Welcome everyone\")\n",
        "print (\"Lets start at 8:05 pm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Agenda:\n",
        "- Introduction\n",
        "- Sentiment analysis -- NLP (Natural language processing)\n",
        "  - Data processing pipeline\n",
        "    - Text cleaning\n",
        "    - Tokenisation\n",
        "    - Stop word removal\n",
        "    - Lemmatisation\n",
        "  - Text format to Numerical format\n",
        "    - BoW\n",
        "    - Word2Vec\n",
        "  - Model for sentiment analysis\n",
        "    - Simple NN\n",
        "    - Seq2seq\n",
        "    - Bert\n",
        "    - GPT\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "rZ2PfcWWV2eT",
        "outputId": "576adddf-163c-4896-9169-4875006d5647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAgenda:\\n- Introduction\\n- Sentiment analysis -- NLP (Natural language processing)\\n  - Data processing pipeline\\n    - Text cleaning\\n    - Tokenisation\\n    - Stop word removal\\n    - Lemmatisation\\n  - Text format to Numerical format\\n    - BoW\\n    - Word2Vec\\n  - Model for sentiment analysis\\n    - Simple NN\\n    - Seq2seq\\n    - Bert\\n    - GPT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Manish Garg\n",
        "Cofounder at Bosscoder\n",
        "2016 Grad -- IIT Dhanbad -- CSE\n",
        "2 years -- Samsung -- Voice intellgence group -- ML Engineer\n",
        "Essential -- Andy Rubin -- Founder of Android -- Voice focussed OS\n",
        "Hamburg, Germany -- Quoori -- ML Lead -- Leading team of 5 ML Engineers\n",
        " - Ondevice ML\n",
        "Bosscoder academy\n",
        "--> Product & Tech\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "13HtMP7qZbQ0",
        "outputId": "9a31423c-2ec3-4e66-8a4f-e5323bdeae5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nManish Garg\\nCofounder at Bosscoder\\n2016 Grad -- IIT Dhanbad -- CSE\\n2 years -- Samsung -- Voice intellgence group -- ML Engineer\\nEssential -- Andy Rubin -- Founder of Android -- Voice focussed OS\\nHamburg, Germany -- Quoori -- ML Lead -- Leading team of 5 ML Engineers\\n - Ondevice ML\\nBosscoder academy\\n--> Product & Tech\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "NLP: Natural langauge processing\n",
        "\n",
        "AI/ML --> Subdomain --> Goal: Understanding of human/natural language\n",
        "\n",
        "NLP is hard --> Natural language or human language is hard to understand.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "LYIMNtNlaFcj",
        "outputId": "59a662a0-91fc-4b71-9f97-033bbc11dbb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNLP: Natural langauge processing\\n\\nAI/ML --> Subdomain --> Goal: Understanding of human/natural language\\n\\nNLP is hard --> Natural language or human language is hard to understand. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Usecases of NLP?\n",
        "- Sentiment analysis\n",
        "- Chatbots/Voice assistants like Alexa, Siri, ChatGPT\n",
        "- Text summarisation\n",
        "- Translator, Machine translation\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "81wL6Un2bKm3",
        "outputId": "b303697e-6630-4f4a-ce2c-e4ef4beb8b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUsecases of NLP? \\n- Sentiment analysis\\n- Chatbots/Voice assistants like Alexa, Siri, ChatGPT\\n- Text summarisation\\n- Translator, Machine translation \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  importing the \"drive\" module from the \"google.colab\" library, facilitating access to Google Drive within a Colab notebook.\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "h3O1SuCKcDZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting the user's Google Drive to the \"/content/drive\" directory in a Google Colab notebook, enabling access to files and data stored on Google Drive within the Colab environment.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D58bHlEcF3U",
        "outputId": "981b23ef-15dc-4b1d-9398-4246a028f57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading a CSV file named \"data.csv\" located in the \"Data\" folder on the your Google Drive and stores it as a DataFrame df\n",
        "# write your data file path\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Master Class/IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "N6QT4eB-cHEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GvGmIaAPcOpf",
        "outputId": "5eed3102-cf74-4a31-cc39-eeaf3b3b8c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1fd63344-7e5b-472b-b205-aeadac47e5ca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fd63344-7e5b-472b-b205-aeadac47e5ca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1fd63344-7e5b-472b-b205-aeadac47e5ca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1fd63344-7e5b-472b-b205-aeadac47e5ca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-46469fe4-91b9-4633-92d9-e79568c7b65c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-46469fe4-91b9-4633-92d9-e79568c7b65c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-46469fe4-91b9-4633-92d9-e79568c7b65c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the TextBlob class from the textblob library\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Assuming you have already mounted Google Drive and loaded the IMDB dataset into a DataFrame df\n",
        "\n",
        "# Limit the analysis to the first 100 reviews\n",
        "num_reviews_to_analyze = 100\n",
        "\n",
        "# Initialize variables for accuracy calculation\n",
        "correct_predictions = 0\n",
        "\n",
        "# Iterate through the first 100 reviews in the dataset\n",
        "for index, row in df.head(num_reviews_to_analyze).iterrows():\n",
        "    # Extract the review text from the 'review' column\n",
        "    review_text = row['review']\n",
        "\n",
        "    # Create a TextBlob object for the current review\n",
        "    analysis = TextBlob(review_text)\n",
        "\n",
        "    # Calculate the sentiment polarity of the review\n",
        "    sentiment_polarity = analysis.sentiment.polarity\n",
        "\n",
        "    # Convert polarity to predicted sentiment label\n",
        "    predicted_sentiment = 'positive' if sentiment_polarity > 0 else 'negative' if sentiment_polarity < 0 else 'neutral'\n",
        "\n",
        "    # Compare with the true sentiment label\n",
        "    true_sentiment = row['sentiment']\n",
        "\n",
        "    # Check if prediction is correct\n",
        "    if predicted_sentiment == true_sentiment:\n",
        "        correct_predictions += 1\n",
        "\n",
        "    # Print the sentiment polarity and predicted sentiment of the review\n",
        "    print(f\"Review {index + 1}: Polarity={sentiment_polarity}, Predicted Sentiment={predicted_sentiment}, True Sentiment={true_sentiment}\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / num_reviews_to_analyze\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19qzVszlcRZ0",
        "outputId": "b940fee5-0c2e-4014-c171-88ec3a5f2764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1: Polarity=0.023433179723502305, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 2: Polarity=0.1097222222222222, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 3: Polarity=0.35400793650793644, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 4: Polarity=-0.0578125, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 5: Polarity=0.2179522497704316, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 6: Polarity=0.15529411764705883, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 7: Polarity=0.2855218855218855, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 8: Polarity=0.08271604938271605, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 9: Polarity=-0.1428628389154705, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 10: Polarity=0.41500000000000004, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 11: Polarity=0.12738095238095237, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 12: Polarity=0.09920634920634923, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 13: Polarity=0.07353896103896104, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 14: Polarity=0.004910714285714289, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 15: Polarity=0.39, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 16: Polarity=-0.046666666666666655, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 17: Polarity=0.23813492063492064, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 18: Polarity=-0.16298076923076923, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 19: Polarity=0.09642857142857143, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 20: Polarity=0.1892156862745098, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 21: Polarity=0.05215517241379311, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 22: Polarity=-0.1618333333333333, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 23: Polarity=0.2642857142857143, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 24: Polarity=-0.02510748510748509, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 25: Polarity=0.039692982456140356, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 26: Polarity=0.06963541666666667, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 27: Polarity=0.18794263496644448, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 28: Polarity=-0.06759259259259259, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 29: Polarity=-0.0732142857142857, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 30: Polarity=-0.021323529411764696, Predicted Sentiment=negative, True Sentiment=positive\n",
            "Review 31: Polarity=0.014071969696969696, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 32: Polarity=0.11227477477477481, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 33: Polarity=0.019871794871794853, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 34: Polarity=0.14265057621222005, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 35: Polarity=0.02719298245614035, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 36: Polarity=0.0243431855500821, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 37: Polarity=-0.38958333333333334, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 38: Polarity=0.0466765873015873, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 39: Polarity=0.4161424512987013, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 40: Polarity=-0.1720627325890484, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 41: Polarity=0.17916666666666664, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 42: Polarity=0.32275641025641033, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 43: Polarity=-0.10548340548340547, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 44: Polarity=0.12791666666666668, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 45: Polarity=0.13842592592592592, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 46: Polarity=0.16928374655647388, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 47: Polarity=0.3625, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 48: Polarity=0.20000000000000004, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 49: Polarity=0.14517077664399092, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 50: Polarity=0.0149122807017544, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 51: Polarity=0.07386937557392104, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 52: Polarity=0.20056128512010857, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 53: Polarity=0.09166666666666666, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 54: Polarity=0.3812152133580705, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 55: Polarity=0.010439560439560446, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 56: Polarity=0.06311111111111116, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 57: Polarity=0.10572562358276642, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 58: Polarity=0.1886671401515152, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 59: Polarity=0.1639059054153394, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 60: Polarity=0.290343300110742, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 61: Polarity=0.12097718253968255, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 62: Polarity=-0.12760416666666669, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 63: Polarity=0.20017857142857146, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 64: Polarity=0.115625, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 65: Polarity=0.0851851851851852, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 66: Polarity=0.15848039215686271, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 67: Polarity=-0.05669973544973544, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 68: Polarity=0.15555555555555556, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 69: Polarity=0.16218434343434346, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 70: Polarity=0.20717687074829932, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 71: Polarity=0.11611374736374737, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 72: Polarity=0.03606532356532357, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 73: Polarity=-0.05405844155844155, Predicted Sentiment=negative, True Sentiment=positive\n",
            "Review 74: Polarity=0.14739583333333336, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 75: Polarity=0.1266628873771731, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 76: Polarity=0.11625, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 77: Polarity=-0.07110215053763441, Predicted Sentiment=negative, True Sentiment=positive\n",
            "Review 78: Polarity=0.0037337662337662367, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 79: Polarity=0.03108766233766233, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 80: Polarity=0.22443064182194622, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 81: Polarity=0.18038194444444444, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 82: Polarity=-0.11022727272727272, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 83: Polarity=0.04107289107289107, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 84: Polarity=-0.07848214285714289, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 85: Polarity=-0.5687500000000001, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 86: Polarity=-0.11963577097505668, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 87: Polarity=0.1, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 88: Polarity=0.19055397727272727, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 89: Polarity=-0.09444444444444444, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 90: Polarity=-0.020014880952380944, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 91: Polarity=0.19631944444444444, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 92: Polarity=0.0019480519480519526, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 93: Polarity=0.2460449735449735, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 94: Polarity=0.10744318181818183, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 95: Polarity=0.08293650793650793, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 96: Polarity=-0.08255834204110067, Predicted Sentiment=negative, True Sentiment=positive\n",
            "Review 97: Polarity=0.0773809523809524, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 98: Polarity=-0.20089613970588233, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 99: Polarity=-0.3800595238095238, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 100: Polarity=0.11050741792929292, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Accuracy: 59.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input --> Text sentences --> Labels\n",
        "Output --> Positive sentiment or Negative sentiment\n",
        "\n",
        "Steps:\n",
        "0. Data collection\n",
        "   - Data collected from users -- Amazon - customer reviews\n",
        "   - Artifically generated data\n",
        "   - Data bought from other companies\n",
        "1. Data processing step\n",
        "2. Convert data into numerical format -- Important\n",
        "3. Build model and evaluate model\n",
        "4. Deploy the model\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "MbpHSk6Lclct",
        "outputId": "e13defa0-5a1d-4c4f-a86d-16ba31e0cdc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nInput --> Text sentences --> Labels\\nOutput --> Positive sentiment or Negative sentiment\\n\\nSteps:\\n0. Data collection\\n   - Data collected from users -- Amazon - customer reviews\\n   - Artifically generated data\\n   - Data bought from other companies\\n1. Data processing step \\n2. Convert data into numerical format -- Important \\n3. Build model and evaluate model\\n4. Deploy the model \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. Data processing\n",
        "\n",
        "- Text cleaning --> Remove punctuations, Remove misspelled words,\n",
        "Replace misspelled ords with correct words.\n",
        "\n",
        "- Tokenisation --> Coverting sentence into list of words\n",
        "\n",
        "I love the food here --> ['I', 'love', 'the', 'food', 'here']\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "omg, d food was ooowasam..... I am in <heart emoji> with this restaurant.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L6j5pMEye3_a",
        "outputId": "f377f8a0-b094-40b7-c225-e67dbe1a3312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nomg, d food was ooowasam..... I am in <heart emoji> with this restaurant. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Natural Language Toolkit (nltk) library\n",
        "import nltk\n",
        "\n",
        "# Download the Punkt tokenizer model (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import the word_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"This is random text.\"\n",
        "\n",
        "# Tokenize the text using word_tokenize\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "# Print the list of tokens\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiUyHtagf23O",
        "outputId": "da1c5a96-982f-4183-b189-0cd12230ab90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'random', 'text', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Lemmatisation --> Process to convert word to their base/root form.\n",
        "\n",
        "1M words -->\n",
        "20 words -->\n",
        "\n",
        "The foxes are waiting for the dogs silently.\n",
        "-->\n",
        "The fox are wait for the dog silent.\n",
        "\n",
        "wait --> wait\n",
        "waited -- wait\n",
        "waiting -- wait\n",
        "....\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "UQJm66mXgGu7",
        "outputId": "8d686c9b-0abd-4689-b40c-4b7f75e292dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLemmatisation --> Process to convert word to their base/root form. \\n\\n1M words -->\\n20 words --> \\n\\nThe foxes are waiting for the dogs silently. \\n--> \\nThe fox are wait for the dog silent. \\n\\nwait --> wait\\nwaited -- wait\\nwaiting -- wait\\n.... \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the WordNetLemmatizer class from the nltk.stem module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Import the word_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Import the Natural Language Toolkit (nltk) library\n",
        "import nltk\n",
        "\n",
        "# Download the WordNet dataset (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Create an instance of the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Lemmatize the tokens\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Print the original tokens and the lemmatized tokens\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFWPfaq1hueE",
        "outputId": "fa431d1e-1c0c-40ff-f163-983d95e95aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', '.']\n",
            "Lemmatized Tokens: ['The', 'quick', 'brown', 'fox', 'are', 'jumping', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Sentence --> Not clean\n",
        "--> Cleaning, tokanisation, lemmatization\n",
        "--> List of words in the base form.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "UfcZki2ViCDU",
        "outputId": "ac8fb154-d086-41f4-91f6-ac9d08d04558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSentence --> Not clean\\n--> Cleaning, tokanisation, lemmatization\\n--> List of words in the base form.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Convert data into numerical format?\n",
        "- BoW - 1 hot encoding\n",
        "- TFIDF\n",
        "- Word2Vec\n",
        "'''\n",
        "\n",
        "'''\n",
        "\n",
        "S1: The movie was great. Action was awesome\n",
        "S2: The acting in the movie was the best part\n",
        "S3: Plot was just awesome\n",
        "\n",
        "The -- 1\n",
        "movie -- 2\n",
        "was -- 3\n",
        "great -- 4\n",
        "Action -- 5\n",
        "Awesome -- 6\n",
        "acting -- 7\n",
        "in -- 8\n",
        "best -- 9\n",
        "part -- 10\n",
        "Plot -- 11\n",
        "just -- 12\n",
        "\n",
        "     The movie was great action awesome acting in best part plot just\n",
        "S1:  [1.   1.    2.   1.    1.     1.     0.     0.  0.   0.  0.    0]\n",
        "S2:  [3.   1     1    0     0      0      1.     1   1.   1.  0.    0 ]\n",
        "S3:  [0.   0     1    0.    0      1      0.     0.  0.   0   1.   1]\n",
        "\n",
        "Unique words --> 1 Lakh -- 100000\n",
        "Avg length of sentence -- 20 -->\n",
        "\n",
        "Disadvanctages of this technique?\n",
        "- Sparcity / Curse of dimensionality\n",
        "- May lose meaning\n",
        "- Seq is lost\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "BPqA5r_ai6IR",
        "outputId": "73e07571-dc27-42d8-e887-10e8a12d3241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nS1: The movie was great. Action was awesome\\nS2: The acting in the movie was the best part\\nS3: Plot was just awesome\\n\\nThe -- 1\\nmovie -- 2\\nwas -- 3\\ngreat -- 4\\nAction -- 5\\nAwesome -- 6\\nacting -- 7\\nin -- 8\\nbest -- 9\\npart -- 10\\nPlot -- 11\\njust -- 12\\n\\n     The movie was great action awesome acting in best part plot just\\nS1:  [1.   1.    2.   1.    1.     1.     0.     0.  0.   0.  0.    0]\\nS2:  [3.   1     1    0     0      0      1.     1   1.   1.  0.    0 ]\\nS3:  [0.   0     1    0.    0      1      0.     0.  0.   0   1.   1]\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Word2Vec\n",
        "- Most powerful innovations in the history of NLP\n",
        "-\n",
        "For every word there will some dimention (Lets say 200 dim/300 dim) vector\n",
        "\n",
        "Notes --> [0.1, -0.7, 0.98, 0.3, 0.01, -0.88...........]\n",
        "The --> [0.2, 0.7, 0.9, -0.3, 0.01, -0.88...........]\n",
        ".\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "GSAb1dmxka9x",
        "outputId": "2160d697-0960-4c2c-d641-c10545007b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWord2Vec \\n- Most powerful innovations in the history of NLP \\n- \\nFor every word there will some dimention (Lets say 200 dim/300 dim) vector\\n\\nNotes --> [0.1, -0.7, 0.98, 0.3, 0.01, -0.88...........]\\nThe --> [0.2, 0.7, 0.9, -0.3, 0.01, -0.88...........]\\n.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already mounted Google Drive and loaded the IMDB dataset into a DataFrame df\n",
        "\n",
        "# Import the CountVectorizer class from the sklearn.feature_extraction.text module\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create an instance of the CountVectorizer class\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Extract the 'review' column from the IMDB dataset as the corpus\n",
        "corpus = df['review'].tolist()\n",
        "\n",
        "# Transform the corpus into a Bag of Words representation\n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Display the feature names (unique words) in the corpus\n",
        "print(\"Feature Names (Unique Words):\", vectorizer.get_feature_names_out()[:20])  # Displaying the first 20 feature names\n",
        "\n",
        "# Display the Bag of Words matrix (showing only the first 5 reviews for brevity)\n",
        "print(\"Bag of Words Matrix:\")\n",
        "print(X_bow[:5].toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhp61TYBodjJ",
        "outputId": "2c8e9076-719d-4de8-91bf-6e8c85ca68e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (Unique Words): ['00' '000' '00000000000' '0000000000001' '00000001' '00001' '00015'\n",
            " '000dm' '000s' '001' '003830' '006' '0069' '007' '0079' '007s' '0080'\n",
            " '0083' '009' '0093638']\n",
            "Bag of Words Matrix:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Word2Vec --> One of the most facinatioing things about NLP\n",
        "'''\n",
        "\n",
        "'''\n",
        "https://stackoverflow.com/questions/56100162/word-mapping-for-2d-word-embedding\n",
        "'''\n",
        "\n",
        "# Assuming you have already mounted Google Drive and loaded the IMDB dataset into a DataFrame df\n",
        "\n",
        "# Import the Word2Vec class from the gensim.models module\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Import the word_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize a smaller sample of the 'review' column from the IMDB dataset\n",
        "sample_size = 1000  # You can adjust the sample size based on your needs\n",
        "tokenized_corpus_sample = [word_tokenize(review.lower()) for review in df['review'].head(sample_size).tolist()]\n",
        "\n",
        "# Create the Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_corpus_sample, vector_size=100, window=5, min_count=1, workers=4)\n",
        "# Adjust the parameters (e.g., vector_size, window) based on your specific requirements.\n",
        "\n",
        "# Get the vector representation of a word (e.g., 'document')\n",
        "vector_representation = word2vec_model.wv['document']\n",
        "\n",
        "# Print the vector representation\n",
        "print(\"Vector representation of 'document':\", vector_representation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcdoDLCuopCx",
        "outputId": "ccac9ce1-7359-481c-e565-c0caf02500d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of 'document': [-2.17751674e-02  1.94484517e-02  5.35718165e-03 -3.79061385e-06\n",
            " -6.56496792e-04 -4.08195481e-02  5.40839229e-03  5.16154543e-02\n",
            " -7.87966140e-03 -1.79936942e-02 -2.95108184e-03 -3.76303717e-02\n",
            "  4.38979920e-03  2.01147404e-02  9.20146890e-03 -2.29805466e-02\n",
            "  1.13361897e-02 -2.38640737e-02 -1.35806780e-02 -1.47369253e-02\n",
            "  7.46932998e-03  1.52735533e-02  2.57625319e-02 -7.15242422e-05\n",
            " -2.88014859e-03  7.22275639e-04 -3.78855206e-02 -3.63251241e-03\n",
            " -1.79222114e-02  9.04277340e-03  1.77197475e-02 -1.60708185e-02\n",
            " -8.07545148e-03 -1.21901911e-02 -2.15454996e-02  1.35026304e-02\n",
            "  1.83027890e-02 -1.57330297e-02  1.01415599e-02 -1.76395252e-02\n",
            "  1.30687114e-02 -1.28006311e-02 -1.18923159e-02 -1.52471745e-02\n",
            "  5.28266840e-03 -6.16984814e-03 -1.02372896e-02  2.28086277e-03\n",
            "  5.32074180e-03  2.82750407e-05  1.74548551e-02 -3.35527770e-02\n",
            "  1.20763704e-02  1.95085595e-03 -1.06957620e-02  2.87658279e-03\n",
            "  2.65505537e-02 -8.18141084e-03 -1.59870405e-02  4.72353771e-03\n",
            "  3.75311030e-03 -1.43106859e-02  5.14603592e-03 -7.17434706e-03\n",
            " -4.66072420e-03  1.14275524e-02 -1.49458479e-02  1.67268533e-02\n",
            " -1.27828745e-02  1.36677930e-02 -8.72619823e-03  1.17082782e-02\n",
            "  2.75550354e-02  1.44319320e-02  2.06867158e-02 -9.22406267e-04\n",
            " -7.58983660e-03  8.71708523e-03 -2.42258087e-02 -6.77382201e-03\n",
            " -2.66929548e-02  1.67462113e-03 -1.22562433e-02  3.25352512e-02\n",
            "  7.26375263e-04  1.90243535e-02  9.34791565e-03  9.32182767e-04\n",
            "  2.25023385e-02  1.52290910e-02  3.23582068e-02  2.32250299e-02\n",
            "  2.23626470e-04 -7.96137471e-03  5.30415587e-02  2.36493833e-02\n",
            " -6.54220860e-03 -1.71552710e-02 -5.49454894e-03 -7.58562412e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "words --> Vectors\n",
        "The fox is jumping over dogs\n",
        "The --> []\n",
        "fox --> [300 dim vector]\n",
        "is --> [300 dim vector]\n",
        "\n",
        "For a sentence --> Vector size will be --> Length of sentence * 200 dim\n",
        "\n",
        "-- No of dimentions affect the results\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1bnw2M49pAnr",
        "outputId": "787de991-7a7c-42dd-b5f6-4e309e274035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwords --> Vectors \\nThe fox is jumping over dogs\\nThe --> []\\nfox --> [300 dim vector]\\nis --> [300 dim vector]\\n\\nFor a sentence --> Vector size will be --> Length of sentence * 200 dim \\n\\n-- No of dimentions affect the results \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Model training\n",
        "\n",
        "2016 -- Seq2Seq -- Encoder decoder based architecture with attention\n",
        "--> Transformer based architecture.\n",
        "\n",
        "My name is Manish. --> Mera nam manish hai\n",
        "I work at Bosscoder --> M bosscoder me kam karta hu.\n",
        "\n",
        "2017 -- Attention is all you need?\n",
        "'''"
      ],
      "metadata": {
        "id": "kp5nPbuyqrSV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}